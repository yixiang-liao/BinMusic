from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence

# å‘é‡åµŒå…¥æ¨¡å‹
embedding = HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")

# è¼‰å…¥å‘é‡è³‡æ–™åº«
vectorstore = FAISS.load_local("./faiss_db_NEWS", embedding, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever(search_kwargs={"k": 20})

# LLM
# llm = OllamaLLM(model="gemma:2b")
# llm = OllamaLLM(model="gemma:7b")
# llm = OllamaLLM(model="llama3")
llm = OllamaLLM(model="mistral")

# PromptTemplate
prompt = PromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä½éŸ³æ¨‚æ–°èå°ç·¨ã€‚

è«‹æ ¹æ“šæä¾›çš„æ–°èè³‡æ–™ï¼Œç”¨ç¹é«”ä¸­æ–‡è©³ç´°å›ç­”ä¸‹æ–¹çš„å•é¡Œã€‚
è«‹å‹™å¿…å¾ã€æ–°èæ¨™é¡Œã€‘èˆ‡ã€æ—¥æœŸã€‘ä¸­æ‰¾ç·šç´¢ä½œç­”ã€‚
å¦‚æœæ–°èä¸­å®Œå…¨æ²’æœ‰ç›¸é—œå…§å®¹ï¼Œå†å›ç­”ã€Œè³‡æ–™ä¸­æœªæåŠã€ã€‚

æ–°èè³‡æ–™å¦‚ä¸‹ï¼š
=========
{context}
=========

å•é¡Œï¼š{question}

è«‹ç”¨æ ¹æ“šé€™äº›æ–°èåšä¸€å€‹ç¶œåˆæ•´ç†æ–‡ç« ã€‚
""")
# prompt = PromptTemplate.from_template("""
# è«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™ï¼Œç”¨ç¹é«”ä¸­æ–‡è©³ç´°å›ç­”å•é¡Œã€‚
# å¦‚æœè³‡æ–™ä¸­æ²’æœ‰æåŠï¼Œè«‹å›ç­”ã€Œè³‡æ–™ä¸­æœªæåŠã€ã€‚

# =========
# {context}
# =========

# å•é¡Œï¼š{question}
# """)

# Chain æ–°å¯«æ³•ï¼ˆé¿å… LLMChain deprecationï¼‰
chain: RunnableSequence = prompt | llm

# æŸ¥è©¢
query = "2025å¹´ éº‹å…ˆç”Ÿæœ€è¿‘æœ‰ä»€éº¼æ–°èæˆ–è¦åŠƒï¼Ÿ"
docs = retriever.invoke(query)
context = "\n\n".join([doc.page_content for doc in docs])

# ğŸ” å°å‡º context ç¢ºèªå‘½ä¸­ä»€éº¼
print("ğŸ” æª¢ç´¢åˆ°çš„å…§å®¹ï¼š")
for i, doc in enumerate(docs):
    print(f"[{i}] {doc.page_content[:100]}...")

# å‘¼å« LLM
result = chain.invoke({"context": context, "question": query})
print("ğŸ§  å›ç­”ï¼š", result)
