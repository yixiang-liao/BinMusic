from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain

# å‘é‡åµŒå…¥æ¨¡å‹
embedding = HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")

# è¼‰å…¥å‘é‡è³‡æ–™åº«
vectorstore = FAISS.load_local("./faiss_db_album", embedding, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})  # å¯ä¾éœ€è¦èª¿æ•´ k

# æœ¬åœ° LLM æ¨¡å‹
# llm = OllamaLLM(model="llama3")
llm = OllamaLLM(model="gemma:2b")

# PromptTemplate
prompt_template = """
è«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™ï¼Œç”¨ç¹é«”ä¸­æ–‡è©³ç´°å›ç­”å•é¡Œã€‚
å¦‚æœè³‡æ–™ä¸­æ²’æœ‰æåŠï¼Œè«‹å›ç­”ã€Œè³‡æ–™ä¸­æœªæåŠã€ã€‚

=========
{context}
=========

å•é¡Œï¼š{question}
"""
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template
)

# åŒ…è£ç‚º LLMChain
llm_chain = LLMChain(prompt=prompt, llm=llm)

# æ¸¬è©¦æŸ¥è©¢
query = "å¯ä»¥æ¨è–¦çµ¦æˆ‘å¹¾é¦–æµªæ¼«çš„æ­Œæ›²å—ï¼Œä¸¦èªªæ˜æ¨è–¦çš„åŸå› "
docs = retriever.get_relevant_documents(query)
context = "\n\n".join([doc.page_content for doc in docs])

# åŸ·è¡Œ
result = llm_chain.run({"context": context, "question": query})

# é¡¯ç¤ºçµæœ
print("ğŸ§  å›ç­”ï¼š", result)
# print("ğŸ“„ ä¾†æºï¼š")
# for doc in docs:
#     print("-", doc.page_content)
