from functools import lru_cache
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from app.db.session import get_db
from app.db.models.artists import Artist
from app.db.models.album import Album , Lyric , LyricLine
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate
from langchain.chains.llm import LLMChain
from app.schemas.rag.rag_response import AskRequest , AskResponse
from pathlib import Path
from fastapi.responses import StreamingResponse
from huggingface_hub import InferenceClient
import asyncio
from app.core.config import Settings
from collections import defaultdict
import json



VECTOR_DB_PATH = (Path(__file__).parent / "faiss_db_album_V2").resolve()        # path to your FAISS index directory
VECTOR_DB_PATH_NEWS = (Path(__file__).parent / "faiss_db_NEWS").resolve()        # path to your FAISS index directory
EMBEDDING_MODEL = "shibing624/text2vec-base-chinese"
LLM_MODEL = "gemma3:4b-it-qat"                          # change if you prefer another Ollama model
TOP_K = 10                                       # number of passages to retrieve

router = APIRouter()

@lru_cache(maxsize=1)
def _get_embeddings():
    return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

# å°ˆè¼¯å‘é‡è³‡æ–™åº«èˆ‡ retriever
@lru_cache(maxsize=1)
def _get_vectorstore_album():
    return FAISS.load_local(VECTOR_DB_PATH, _get_embeddings(), allow_dangerous_deserialization=True)

@lru_cache(maxsize=1)
def _get_retriever_album():
    return _get_vectorstore_album().as_retriever(search_kwargs={"k": TOP_K})

# æ–°èå‘é‡è³‡æ–™åº«èˆ‡ retriever
@lru_cache(maxsize=1)
def _get_vectorstore_news():
    return FAISS.load_local(VECTOR_DB_PATH_NEWS, _get_embeddings(), allow_dangerous_deserialization=True)

@lru_cache(maxsize=1)
def _get_retriever_news():
    return _get_vectorstore_news().as_retriever(search_kwargs={"k": TOP_K})



@lru_cache(maxsize=1)
def _get_llm_chain():
    prompt_template = (
        """
        è«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™ï¼Œç”¨ç¹é«”ä¸­æ–‡è©³ç´°å›ç­”å•é¡Œã€‚
        å¦‚æœè³‡æ–™ä¸­æ²’æœ‰æåŠï¼Œè«‹å›ç­”ã€Œè³‡æ–™ä¸­æœªæåŠã€ã€‚
        =========\n{context}\n=========
        \nå•é¡Œï¼š{question}
        """
    )
    prompt = PromptTemplate(
        input_variables=["context", "question"], template=prompt_template
    )
    llm = OllamaLLM(model=LLM_MODEL, streaming=True)
    return LLMChain(prompt=prompt, llm=llm)

@lru_cache(maxsize=1)
def _get_llm_chainï¼¿NEWS():
    # prompt_template = ("""
    #     ä½ æ˜¯ä¸€ä½å°ç£æµè¡ŒéŸ³æ¨‚çŸ¥è­˜åº«çš„å°ç·¨ï¼Œæ“…é•·æ•´åˆè³‡æ–™ä¸¦æ¸…æ¥šå›ç­”å•é¡Œã€‚

    #     æ ¹æ“šä»¥ä¸‹è³‡æ–™ï¼Œè«‹ç¸½åˆæ•´ç†ä¸¦ç”¨ç¹é«”ä¸­æ–‡æœ‰æ¢ç†åœ°å›ç­”å•é¡Œã€‚
    #     è‹¥è³‡æ–™ä¸­æœªæåŠï¼Œè«‹æ˜ç¢ºæŒ‡å‡ºã€Œè³‡æ–™ä¸­æœªæåŠã€ï¼Œä¸è¦è‡†æ¸¬ã€‚

    #     ========= è³‡æ–™é–‹å§‹ =========
    #     {context}
    #     ========= è³‡æ–™çµæŸ =========

    #     å•é¡Œï¼š{question}

    #     è«‹è©³ç´°ä½œç­”ï¼š
    #     """
    # )
    prompt_template = ("""
        ä½ æ˜¯ä¸€ä½å°ç£æµè¡ŒéŸ³æ¨‚çŸ¥è­˜åº«çš„å°ç·¨ï¼Œæ“…é•·æ•´åˆè³‡æ–™ä¸¦æ¸…æ¥šå›ç­”å•é¡Œã€‚

        æ ¹æ“šä»¥ä¸‹è³‡æ–™ï¼Œè«‹ç¸½åˆæ•´ç†ä¸¦ç”¨ç¹é«”ä¸­æ–‡æœ‰æ¢ç†åœ°å›ç­”å•é¡Œã€‚

        ========= è³‡æ–™é–‹å§‹ =========
        {context}
        ========= è³‡æ–™çµæŸ =========

        å•é¡Œï¼š{question}

        è«‹è©³ç´°ä½œç­”ï¼š
        """
    )
    prompt = PromptTemplate(
        input_variables=["context", "question"], template=prompt_template
    )
    llm = OllamaLLM(model=LLM_MODEL, streaming=True)
    return LLMChain(prompt=prompt, llm=llm)

@router.get("/health", summary="å¥åº·æª¢æŸ¥")
async def health_check():
    return {"status": "ok"}

@router.post("/stream/ask/album", summary="ä¸²æµå›ç­”å°ˆè¼¯")
async def ask_stream_album(req: AskRequest):
    retriever = _get_retriever_album()
    llm_chain = _get_llm_chain()

    docs = retriever.get_relevant_documents(req.question)

    # æ•´ç†ä¾†æº
    sources = [
        {
            "title": doc.metadata.get("title", "æœªçŸ¥æ¨™é¡Œ"),
            "content": doc.page_content[:200]  # ç¯€éŒ„å‰æ®µæ–‡å­—
        }
        for doc in docs
    ]

    print("ğŸ” [ALBUM] æª¢ç´¢åˆ°çš„æ–‡ä»¶ï¼ˆåŸå§‹æ®µè½ï¼‰:")
    for i, doc in enumerate(docs, 1):
        print(f"--- Document {i} ---")
        print(f"parent_id: {doc.metadata.get('parent_id')}")
        print(f"title: {doc.metadata.get('title', 'N/A')}")
        print(f"å…§å®¹:\n{doc.page_content}")
        print(doc.page_content[:500], "\n")

    context = "\n\n".join(doc.page_content for doc in docs)

    async def token_stream():
        # å›å‚³ä¾†æºè³‡æ–™ï¼ˆtype: sourceï¼‰
        yield json.dumps({
            "type": "source",
            "data": sources
        }) + "\n"

        # ä¸²æµå›ç­”å…§å®¹ï¼ˆtype: answerï¼‰
        async for chunk in llm_chain.astream({"context": context, "question": req.question}):
            yield json.dumps({
                "type": "answer",
                "data": chunk["text"]
            }) + "\n"
            await asyncio.sleep(0.01)

    return StreamingResponse(token_stream(), media_type="text/plain")


@router.post("/stream/ask/news", summary="ä¸²æµå›ç­”æ–°èç›¸é—œ")
async def ask_stream_news(req: AskRequest):
    retriever = _get_retriever_news()
    llm_chain = _get_llm_chain_NEWS()

    docs = retriever.get_relevant_documents(req.question)

    print("ğŸ” [NEWS] æª¢ç´¢åˆ°çš„æ–‡ä»¶ï¼ˆåŸå§‹æ®µè½ï¼‰:")
    for i, doc in enumerate(docs, 1):
        print(f"--- Document {i} ---")
        print(f"parent_id: {doc.metadata.get('parent_id')}")
        print(f"title: {doc.metadata.get('title', 'N/A')}")
        print(f"å…§å®¹:\n{doc.page_content[:300]}...\n")  # æœ€å¤šå°å‰æ®µ

    # æå–ä¾†æºè³‡è¨Š
    sources = [
        {
            "title": doc.metadata.get("title", "æœªçŸ¥æ¨™é¡Œ"),
            "content": doc.page_content[:200]  # åƒ…ç¯€éŒ„å‰æ®µæ–‡å­—
        }
        for doc in docs
    ]

    # æ•´åˆ context
    context = "\n\n".join(doc.page_content for doc in docs)

    async def token_stream():
        # å›å‚³ä¾†æºè³‡è¨Šï¼ˆä¸€æ¬¡ï¼‰
        yield json.dumps({
            "type": "source",
            "data": sources
        }) + "\n"

        # ä¸²æµå›å‚³å›ç­”æ–‡å­—
        async for chunk in llm_chain.astream({
            "context": context,
            "question": req.question
        }):
            yield json.dumps({
                "type": "answer",
                "data": chunk["text"]
            }) + "\n"
            await asyncio.sleep(0.01)

    return StreamingResponse(token_stream(), media_type="application/json")